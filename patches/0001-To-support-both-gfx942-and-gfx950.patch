From 0e1639f88b4a1f1737c9a09c66774ad7f379bb35 Mon Sep 17 00:00:00 2001
From: "Lin, Soga" <soga.lin@amd.com>
Date: Wed, 21 May 2025 06:01:27 +0000
Subject: [PATCH] To support both gfx942 and gfx950

---
 aiter/fused_moe_bf16_asm.py      | 8 ++++----
 aiter/jit/optCompilerConfig.json | 8 ++++----
 2 files changed, 8 insertions(+), 8 deletions(-)

diff --git a/aiter/fused_moe_bf16_asm.py b/aiter/fused_moe_bf16_asm.py
index fd310590..e9facfed 100755
--- a/aiter/fused_moe_bf16_asm.py
+++ b/aiter/fused_moe_bf16_asm.py
@@ -140,8 +140,8 @@ def asm_moe(
             128,
         ), "asm_moe for block_scale only support (128, 128)"
         assert (
-            w1.dtype == torch.float8_e4m3fnuz
-        ), "asm_moe for block_scale only support float8_e4m3fnuz weight"
+            w1.dtype == dtypes.fp8
+        ), "asm_moe for block_scale only support float8_e4m3fnuz weight on gfx942"
         assert (
             w2.shape[2] * 2 == w1.shape[1]
         ), "aiter moe for block_scale only support g1u1"
@@ -150,7 +150,7 @@ def asm_moe(
 
         a1_q, a1_scale = pertoken_quant(
             hidden_states.view(-1, model_dim // scale_blk_k, scale_blk_k),
-            quant_dtype=torch.float8_e4m3fnuz,
+            quant_dtype=dtypes.fp8,
         )
         a1_q = a1_q.view(-1, model_dim)
         a1_scale = a1_scale.squeeze(-1).t().contiguous()
@@ -429,7 +429,7 @@ def ck_moe_2stages(
 ):
 
     quant_func = get_hip_quant(quant_type)
-    q_dtype_a = w1.dtype if w1.dtype != torch.uint32 else torch.float8_e4m3fnuz
+    q_dtype_a = w1.dtype if w1.dtype != torch.uint32 else dtypes.fp8
 
     # quant_func = get_torch_quant(quant_type)
     E, model_dim, inter_dim = w2.shape
diff --git a/aiter/jit/optCompilerConfig.json b/aiter/jit/optCompilerConfig.json
index 20d2c67b..74feccc0 100644
--- a/aiter/jit/optCompilerConfig.json
+++ b/aiter/jit/optCompilerConfig.json
@@ -45,7 +45,7 @@
         "flags_extra_cc": [],
         "flags_extra_hip": [
             "'-DENABLE_FP8'",
-            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 0)}'"
+            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 1)}'"
         ],
         "extra_ldflags": "None",
         "extra_include": [
@@ -605,7 +605,7 @@
         "flags_extra_hip": [
             "'-DCK_TILE_FMHA_FWD_FAST_EXP2=1'",
             "f'-DCK_TILE_FLOAT_TO_BFLOAT16_DEFAULT={os.environ.get(\"CK_TILE_FLOAT_TO_BFLOAT16_DEFAULT\", 2)}'",
-            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 0)}'"
+            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 1)}'"
         ],
         "extra_ldflags": "None",
         "extra_include": [
@@ -628,8 +628,8 @@
         "flags_extra_hip": [
             "'-DCK_TILE_FMHA_FWD_FAST_EXP2=1'",
             "f'-DCK_TILE_FLOAT_TO_BFLOAT16_DEFAULT={os.environ.get(\"CK_TILE_FLOAT_TO_BFLOAT16_DEFAULT\", 2)}'",
-            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 0)}'",
-            "f'-DCK_TILE_ATTENTION_USE_SOFTSIGN_ASM={os.environ.get(\"CK_TILE_ATTENTION_USE_SOFTSIGN_ASM\", 1)}'"
+            "f'-DCK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT={os.environ.get(\"CK_TILE_ATTENTION_LOGITS_SOFT_CAP_DEFAULT\", 1)}'",
+            "f'-DCK_TILE_ATTENTION_USE_SOFTSIGN_ASM={os.environ.get(\"CK_TILE_ATTENTION_USE_SOFTSIGN_ASM\", 0)}'"
         ],
         "extra_ldflags": "None",
         "extra_include": [
-- 
2.34.1

